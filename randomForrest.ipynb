{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lime: Explaining predictions \n",
    "\n",
    "### What is it ?\n",
    "\n",
    "Local Interpretable Model-Agnostic Explanations (LIME) is a technique for explaining the decisions made by a machine learning model. The designers of LIME state a goal of identifying an interpretable model over the interpretable representation that is locally faithful to the classifier. The authors define their concept of interpretable representations as human understandable analogs for features used in real world models. With the LIME algorithm the authors hope to “explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model”.  This means that using LIME a simple model that is easy to understand is used to explain the predictions of a more complex model in a localized region. \n",
    "\n",
    "## How to use lime \n",
    "\n",
    "### Instalation \n",
    "\n",
    "LIME is disttributed as a pyhton package instalable with pip or can be downloaded directly from the projects [repository](https://github.com/marcotcr/lime) current versions of the library only support python 3 \n",
    "\n",
    "~~~ bash\n",
    "pip install lime \n",
    "~~~\n",
    "\n",
    "### Basic Usage \n",
    "\n",
    "LIME has methods for explaining many different types of Model for this basic tutorial we will use the tool to explain the predictions of a random forest classifier. first we have to import LIME in the normal way \n",
    "\n",
    "~~~ python\n",
    "import lime \n",
    "\n",
    "~~~\n",
    "\n",
    "The first step in using LIME is initialising the LimeTextExplainer class with the class_names variable for the different classes the explainer will be identifying. LIME has many explainer methods that can be used to explain different models \n",
    "\n",
    "\n",
    "~~~ python \n",
    "lime_explainer = lime_text.LimeTextExplainer(class_names=[\"positive\",\"negative\"])\n",
    "~~~\n",
    "\n",
    "Once the Explainer is intialized it can be used to explain a prediction made by a model. For some models the data is pre-processed into vectorized sets (this is the case for some sklearn models) in these cases we may need to set up a pipline to get access to the unprocessed text data. The max number of features also needs to be apllied to the explainer.\n",
    "\n",
    "\n",
    "~~~ python \n",
    "x = 42 # row of data to be classified and explained \n",
    "explaination = explainer.explain_instance(data[x], classfier, num_features=6)\n",
    "~~~\n",
    "\n",
    "The `explaination` object returned by `explain_instance` contains a linnear aproximation of the classifier for the provided data row. This can be used in a number of ways to comunicate the explaination to the user \n",
    "\n",
    "\n",
    "### Types of Explainer\n",
    "\n",
    "As mentioned above there are several different LIME explainer classes used for different types of data. We have already seen `lime_text` , but the class also includes `lime_tabular` and `lime_images` for tabular and image data respectivly. \n",
    "\n",
    "\n",
    "###  `lime_images`\n",
    "\n",
    "We can seperatly import the diferent lime components using the `from` import pattern. Here we import the lime images component.We will use an example from the lime documentation.\n",
    "\n",
    "~~~ python \n",
    "from lime import lime_image\n",
    "~~~\n",
    "\n",
    "For the image explainer it is not nessasary to initialize the explainer with lables like the text explainer \n",
    "\n",
    "~~~ python \n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "~~~\n",
    "\n",
    "Building an explaination for an image instance requires different parameters than the explainer for text models.\n",
    "This example taken from the LIME documentation is explaining a model which idetifies cats and dogs in an image.\n",
    "we can see that for this explainer we need to pass the image and the model in addition to the `top_labels` parameter which controls the number of labels the explainer will return. `hide_color` which is the colour for a super pixel to be disabled, this value can also be `None`. `num_samples` is the size of the neighborhood to learn the linear model.\n",
    "\n",
    "~~~ python \n",
    "explanation = explainer.explain_instance(image, predict_fn, top_labels=5, hide_color=0, num_samples=1000)\n",
    "~~~\n",
    "\n",
    "Like the explain instance for `lime_text` this instance contains a linear model of the local being explained. However, the explaination produced has different methods for displaying the explaination includig mathods for masking and superimposing colour over the classified image. \n",
    "\n",
    "\n",
    "###  `lime_tabular`\n",
    "\n",
    "The `lime_tabular` explainer is used to explain predictions on matrix data. This explainer differs from the `lime_text` and `lime_images` explainers in that it requires a training set in order to instantiate the explainer. we will use an example from the LIME documentation to demonstarte this class. \n",
    "first we import the class in the usual way.\n",
    "\n",
    "~~~ python \n",
    "from lime import lime_tabular\n",
    "~~~\n",
    "\n",
    "Then we instatniate the explainer the first argument is the training data used to calculate statistics use in processing the model. Then we need to pass the list of feature names coresponding to the columns in the training data and a list of class names. The final argument `discretize_contious` if set to true wil discitize all non-catagorical features.  \n",
    "\n",
    "~~~ python\n",
    "explainer = lime_tabular.LimeTabularExplainer(train, feature_names=feature_names, class_names=target_names, discretize_continuous=True)\n",
    "~~~\n",
    "\n",
    "\n",
    "finally we create the explaination by calling the explain instace method on the test data and the model, here we also specify the number of features and the top labels to be used in the explaination. \n",
    "\n",
    "~~~ python \n",
    "explaination = explainer.explain_instance(test[x], model, num_features=2, top_labels=1)\n",
    "~~~ \n",
    "\n",
    "### Explaining Instances \n",
    "As we have seen LIME has many methods for explaining different models, the real power of the library comes from the out put of the explainers that cn be presented to users in order to give them a better view of what decisions the model is making and why. All of the explainers that we have seen produce a LIME explaination object which has a number of methods for returning different types of data to the user. \n",
    "\n",
    "### `as_list`\n",
    "\n",
    "This method returns the explaination as a python list of tuples with the feature and the wieght the actual values depend based on the particular explainer. the follwing example from the LIME documentation shows the data from a text explainer on a random forest model. \n",
    "\n",
    "~~~ python \n",
    "\n",
    "explaination.as_list()\n",
    "#output \n",
    "[(u'Posting', -0.15748303818990594),\n",
    " (u'Host', -0.13220892468795911),\n",
    " (u'NNTP', -0.097422972255878093),\n",
    " (u'edu', -0.051080418945152584),\n",
    " (u'have', -0.010616558305370854),\n",
    " (u'There', -0.0099743822272458232)]\n",
    "\n",
    "~~~\n",
    "\n",
    "we see that a list of features and weights is produced that coresponds to the linear model of the localixed aproximation. This data can be futher processed or simly displayed to the user \n",
    "\n",
    "### `as_map`\n",
    "\n",
    "this method is similar to as list but it returns a python map of the labels to a list of tuples \n",
    "\n",
    "~~~ python \n",
    "explaination.as_map()\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "### `as_pyplot_figure`\n",
    "\n",
    "This method returns a pyplot barchart figure. This method requires matplotlib to be installed also this method will be ignored for regression explaination instances. The following example comes from the LIME documentation and displays the same data as the `as list` method above. \n",
    "\n",
    "~~~ python \n",
    "figure = explaination.as_pyplot_figure()\n",
    "~~~\n",
    "\n",
    "![images/pyplotLime.png](./images/pyplotLime.png)\n",
    "\n",
    "\n",
    "we can see that the simple methods availiable in lime for quickly produce a graphical intepretation of the explainer that can be presented to users or used in interactive development.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `as_html, save_to_file and show_in_notebook`  \n",
    "\n",
    "Theas methods return the explaination as a html page for easy embeding in web apps. The `save_to_file` method saves the html representation to a file for later use. The `show_in_notebook` method displays the html in an ipython notebook  This method produces bar charts for the different explainations. The following exampls come from the LIME documentation  \n",
    "\n",
    "~~~ python \n",
    "explaination.show_in_notebook(text=False)\n",
    "~~~\n",
    "\n",
    "\n",
    "![images/htmlLime.png](./images/htmllime.png)\n",
    "\n",
    "\n",
    "The previous image is produced by a text explainer, with text explainers it is possible to generate a document with the classified text highlighted \n",
    "\n",
    "~~~ python\n",
    "explaination.show_in_notebook(text=True)\n",
    "~~~\n",
    "\n",
    "![images/htmltextLime.png](./images/htmltextlime.png)\n",
    "\n",
    "\n",
    "\n",
    "### `get_image_and_mask`\n",
    "\n",
    "LIME image explainers inlclude methods to demonstrate the areas of an image that were used for the classification being explained. Thease methods are incredibly powerful for visually demonstrating how the selected image is being clasisified and can be used to quicly identify surios identifications and other problems. The folowing examples from the LIME documentation show some of the methods that can be used. \n",
    "\n",
    "\n",
    "The following example sets the `positive_only` value to true which \n",
    "which only displays positive results and `hide_rest` which applies a grey mask to the image only displaying the interesting area. \n",
    "\n",
    "~~~ python \n",
    "explanation.get_image_and_mask(240, positive_only=True, num_features=5, hide_rest=True)\n",
    "~~~\n",
    "\n",
    "![images/dogmasklime.png](./images/dogmasklime.png)\n",
    "\n",
    "\n",
    "We can also display the same image with no mask on the background image. \n",
    "\n",
    "~~~ python \n",
    "explanation.get_image_and_mask(240, positive_only=True, num_features=5, hide_rest=False)\n",
    "~~~\n",
    "\n",
    "![images/dognomasklime.png](./images/dognomasklime.png)\n",
    "\n",
    "\n",
    "Finaly we can also generate image that highlight the pro and con areas (or combinations therof) in red and greenby setting the `positive_only` parameter to false. \n",
    "\n",
    "~~~ python \n",
    "\n",
    "explanation.get_image_and_mask(240, positive_only=False, num_features=10, hide_rest=False)\n",
    "~~~\n",
    "\n",
    "![images/dogcatlime.png](./images/dogcatlime.png)\n",
    "\n",
    "### Use Cases \n",
    "\n",
    "We have seen the power that the LIME library has to explain how models are coming to their conclusions and demonstrate this to users. There are many use cases for LIME from including an explaination of a classification to an end user of a product so they can have greater confiddence in the predictions made by the model to developers using LIME to identify issues with their code or assumtions that are negativly affecting their work. LIME is a usefull tool in any datascientists tool kit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http response: 200 OK\n",
      "\n",
      "\n",
      "[[164  35]\n",
      " [ 37 164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.82      0.82       199\n",
      "    positive       0.82      0.82      0.82       201\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "0.82\n"
     ]
    }
   ],
   "source": [
    "#load data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "import tarfile\n",
    "import os \n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "url = \"https://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz\"\n",
    "filename = \"processed_acl\"\n",
    "extension = \".tar.gz\"\n",
    "\n",
    "response = requests.get(url, allow_redirects=True)\n",
    "\n",
    "print(\"http response:\",response.status_code,response.reason)\n",
    "\n",
    "with open(filename+extension,\"wb\") as file: \n",
    "    file.write(response.content)\n",
    "    \n",
    "tar = tarfile.open(filename+extension, \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "# # merge files using shell commands \n",
    "print(os.popen(\"cat \" + filename+\"/books/negative.review > mixed.txt\").read())\n",
    "print(os.popen(\"cat \" + filename+\"/books/positive.review >> mixed.txt\").read())\n",
    "# print(os.popen(\"cat \" + filename+\"/dvd/all.review >> mixed.txt\").read())\n",
    "# print(os.popen(\"cat \" + filename+\"/dvd/positive.review >> mixed.txt\").read())\n",
    "\n",
    "\n",
    "\n",
    "# parse data into array for word 2 vect \n",
    "dataArray = []\n",
    "labelArray = []\n",
    "\n",
    "with open(\"mixed.txt\", \"r\") as datafile: \n",
    "    lines = datafile.readlines()\n",
    "    for e in lines:\n",
    "        lineList= []\n",
    "        splitLine = e.split()\n",
    "        label = splitLine.pop(-1)\n",
    "        for i in splitLine:\n",
    "            dictSplit = i.split(':')\n",
    "            lineList.append(dictSplit[0])\n",
    "        dataArray.insert(-1, lineList)\n",
    "        labelArray.insert(-1, (label.split(':')[1]))\n",
    "\n",
    "#download word2vec model          \n",
    "word2vec = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "\n",
    "# create mean embedding \n",
    "embed = np.array([ np.mean([word2vec[w] for w in words if w in word2vec], axis=0) for words in dataArray])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embed, labelArray, test_size=0.2, random_state=42)\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=700, random_state=42)\n",
    "text_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = text_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dl tubspam data set \n",
    "# load into numpy array \n",
    "# create embedings on full text \n",
    "# use classifier to classify the dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lime explainer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
