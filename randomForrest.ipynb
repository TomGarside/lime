{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/.local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http response: 200 OK\n",
      "\n",
      "\n",
      "[[200]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       1.00      1.00      1.00       200\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#load data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "import tarfile\n",
    "import os \n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "url = \"https://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz\"\n",
    "filename = \"processed_acl\"\n",
    "extension = \".tar.gz\"\n",
    "\n",
    "response = requests.get(url, allow_redirects=True)\n",
    "\n",
    "print(\"http response:\",response.status_code,response.reason)\n",
    "\n",
    "with open(filename+extension,\"wb\") as file: \n",
    "    file.write(response.content)\n",
    "    \n",
    "tar = tarfile.open(filename+extension, \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "# # merge files using shell commands \n",
    "print(os.popen(\"cat \" + filename+\"/books/all.review > mixed.txt\").read())\n",
    "print(os.popen(\"cat \" + filename+\"/books/positive.review >> mixed.txt\").read())\n",
    "# print(os.popen(\"cat \" + filename+\"/dvd/all.review >> mixed.txt\").read())\n",
    "# print(os.popen(\"cat \" + filename+\"/dvd/positive.review >> mixed.txt\").read())\n",
    "\n",
    "\n",
    "\n",
    "# parse data into array for word 2 vect \n",
    "dataArray = []\n",
    "labelArray = []\n",
    "\n",
    "with open(\"mixed.txt\", \"r\") as datafile: \n",
    "    lines = datafile.readlines()\n",
    "    for e in lines:\n",
    "        lineList= []\n",
    "        splitLine = e.split()\n",
    "        label = splitLine.pop(-1)\n",
    "        for i in splitLine:\n",
    "            dictSplit = i.split(':')\n",
    "            lineList.append(dictSplit[0])\n",
    "        dataArray.insert(-1, lineList)\n",
    "        labelArray.insert(-1, (label.split(':')[1]))\n",
    "\n",
    "#download word2vec model          \n",
    "word2vec = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "\n",
    "# create mean embedding \n",
    "embed = np.array([ np.mean([word2vec[w] for w in words if w in word2vec], axis=0) for words in dataArray])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embed, labelArray, test_size=0.2, random_state=42)\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=700, random_state=42)\n",
    "text_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = text_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'keySet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-48f990af56d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbest_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubTrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataArray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeySet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keySet' is not defined"
     ]
    }
   ],
   "source": [
    "# random forest \n",
    "from math import log \n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "\n",
    "# calculate entropy \n",
    "def entropy(probabilities):\n",
    "    return sum(-p * log(p,2) for p in probabilities if p )\n",
    "\n",
    "def classProbabilities(classes):\n",
    "    return [count / len(classes) for count in Counter(classes).values()]\n",
    "\n",
    "def get_entropy(data):\n",
    "    return entropy(classProbabilities([label for _, label in data]))\n",
    "\n",
    "def calcPartEntropy(set):\n",
    "    total = sum(len(subset) for subset in set)\n",
    "    \n",
    "    return sum( get_entropy(subset) * len(subset) / total for subset in set)\n",
    "\n",
    "def partitionSet(set, feature):\n",
    "    groups = defaultdict(list)\n",
    "    for subset in set:\n",
    "        if feature in subset[0]:\n",
    "            key = subset[0][feature]\n",
    "            groups[key].append(subset)\n",
    "    return groups \n",
    "\n",
    "def partittion_entropy(set,feature):\n",
    "    partitions = partitionSet(set,feature)\n",
    "    return calcPartEntropy(partitions.values())\n",
    "    \n",
    "    \n",
    "print(partittion_entropy(dataArray,\"shallow\"))\n",
    "    \n",
    "\n",
    "# build tree \n",
    "\n",
    "def build_tree(input_set, feature_split = None):\n",
    "    #print(len(input_set))\n",
    "    # if no split feature selected the first \n",
    "    #print(feature_split)\n",
    "    if not feature_split:\n",
    "        feature_split = input_set[0][0].keys()\n",
    "    #print(len(feature_split))\n",
    "    # check for leaves \n",
    "    setSize = len(input_set)\n",
    "    positive_count = 0 \n",
    "    negative_count = 0 \n",
    "    for e in input_set:\n",
    "        if e[1] == \"positive\":\n",
    "            positive_count+=1 \n",
    "        else:\n",
    "            negative_count+=1\n",
    "    print(\"positive count:\",positive_count,\"negative count:\",negative_count)\n",
    "#     if positive_count == 0:\n",
    "#         return \"positive\"\n",
    "#     if negative_count == 0: \n",
    "#         return \"negative\"\n",
    "    \n",
    "    if not feature_split:\n",
    "        if positive_count > negative_count:\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\"\n",
    "        \n",
    "    best_feature = min(feature_split, key = partial(partittion_entropy,input_set))\n",
    "    #print (feature_split)\n",
    "    \n",
    "    partitions = partitionSet(input_set, best_feature)\n",
    "    \n",
    "    #print(partitions)\n",
    "    new_split_features = [e for e in feature_split if e != best_feature]\n",
    "    \n",
    "    #print(new_split_features)\n",
    "    \n",
    "    subTrees = {attribute_value : build_tree(subset,new_split_features)\n",
    "               for attribute_value, subset in partitions.items()}\n",
    "    print(subTrees)\n",
    "    if not subTrees:\n",
    "         if positive_count > negative_count:\n",
    "            return \"positive\"\n",
    "         else:\n",
    "            return \"negative\"\n",
    "    \n",
    "    return (best_feature, subTrees)\n",
    "\n",
    "tree = build_tree(dataArray,list(keySet))\n",
    "print(tree)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
